{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b10b7e-e399-4beb-b696-fa1a0727adf8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This code module is designed to convert a given text into a structured table. It uses Byte-Pair Encoding for preprocessing, then employs the Fairseq library for table generation. The results can be presented in markdown format for better visualization. To use this module effectively, ensure that all necessary data, models, and dependencies are correctly set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218541d9-6c9e-47e5-8a83-8e8fa92756b6",
   "metadata": {},
   "source": [
    "## Dependencies and Constants\n",
    "Before diving into the logic, the code begins by importing necessary modules and defining some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42fd4ea3-109f-4c80-b9a3-e0d1edd3828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from fairseq.data.encoders.gpt2_bpe import get_encoder\n",
    "\n",
    "# Constants for preprocess_text\n",
    "DEFAULT_WORKERS = 60\n",
    "SPLIT = \"test\"\n",
    "LANG = \"text\"\n",
    "DATA_DIR = os.getcwd()+\"/text_to_table/data/testdata\"\n",
    "CKPT = os.getcwd()+\"/text_to_table/checkpoints/checkpoint_average_best-3.pt\"\n",
    "\n",
    "# Constants for table_generation\n",
    "DEFAULT_BEAM = 5\n",
    "DEFAULT_BUFFER_SIZE = 1024\n",
    "DEFAULT_MAX_TOKENS = 4096\n",
    "DEFAULT_USER_DIR = \"text_to_table/src/\"\n",
    "DEFAULT_TASK = \"text_to_table_task\"\n",
    "DEFAULT_TABLE_MAX_COLUMNS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be507f-e601-44cb-822c-61c76a5b7b3e",
   "metadata": {},
   "source": [
    "## BPE Encoding Utilities\n",
    "Byte-Pair Encoding (BPE) is a type of subword tokenization method. This section contains utility functions and a class to handle BPE encoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d86fbc-9fc6-430c-8ca1-9128c967d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiprocessingEncoder(object):\n",
    "    def __init__(self, encoder_json, vocab_bpe):\n",
    "        self.encoder = get_encoder(encoder_json, vocab_bpe)\n",
    "\n",
    "    def encode(self, line):\n",
    "        ids = self.encoder.encode(line)\n",
    "        return list(map(str, ids))\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.encoder.decode(tokens)\n",
    "\n",
    "    def encode_line(self, line):\n",
    "        \"\"\"\n",
    "        Encode a single line.\n",
    "        \"\"\"\n",
    "        line = line.strip().replace(\" <NEWLINE> \", \"\\n\")\n",
    "        tokens = self.encode(line)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def decode_line(self, line):\n",
    "        tokens = map(int, line.strip().split())\n",
    "        return self.decode(tokens)\n",
    "\n",
    "def encode_text_with_bpe(text, encoder_json, vocab_bpe):\n",
    "    \"\"\"\n",
    "    Encode a single text string using BPE.\n",
    "\n",
    "    Parameters:\n",
    "    - text: The string text to encode.\n",
    "    - encoder_json: Path to the encoder.json file.\n",
    "    - vocab_bpe: Path to the vocab.bpe file.\n",
    "\n",
    "    Returns:\n",
    "    - Encoded text as a string.\n",
    "    \"\"\"\n",
    "    encoder = MultiprocessingEncoder(encoder_json, vocab_bpe)\n",
    "    return encoder.encode_line(text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fb696-bed8-4d14-af14-8d3136dc9024",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions\n",
    "This section handles the preprocessing of the input text to make it suitable for table generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c60b2f-fc33-4242-8798-0ac94efbb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, data_dir):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text using BPE encoding and fairseq-preprocess.\n",
    "\n",
    "    Parameters:\n",
    "    - text: The text to preprocess.\n",
    "    - data_dir: Directory for input and output data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Encode the text using BPE encoding\n",
    "        encoded_text = encode_text_with_bpe(text, os.path.join(data_dir, \"encoder.json\"), os.path.join(data_dir, \"vocab.bpe\"))\n",
    "        \n",
    "        # Write the encoded text to a file\n",
    "        output_file_path = os.path.join(data_dir, f\"{SPLIT}.bpe.{LANG}\")\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            file.write(encoded_text)\n",
    "        \n",
    "        return output_file_path  # Return the path of the preprocessed file\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during preprocessing\n",
    "        print(\"An error occurred during preprocessing:\", str(e))\n",
    "        return str(e)  # Return None to indicate an error   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44947466-c0e9-487e-9ea9-53c3e99bceb0",
   "metadata": {},
   "source": [
    "## Table Generation and Formatting\n",
    "This section is dedicated to converting the preprocessed text into a structured table using the Fairseq library, post-processing the output, and then formatting the result into markdown for better visualization.\n",
    "\n",
    "The function table_generation uses the Fairseq library to generate a table from the preprocessed text. It runs the Fairseq interactive command and then post-processes the output to extract the final table content.\n",
    "\n",
    "Once the table is generated, the next step is to make the output readable. The function convert_fairseq_output_to_text takes the raw output from Fairseq, extracts the hypothesis, and then decodes it using the GPT-2 decoding method to provide a clearer representation of the table.\n",
    "\n",
    "For better presentation, especially on platforms that support markdown rendering, the function convert_to_markdown is used. It takes the generated table string and converts it into a markdown table format, making it easier to read and understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "098dbb19-6fd5-4a92-9ab9-9a7e30960bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_generation(data_path, ckpt):\n",
    "    \"\"\"\n",
    "    Generate table from the text using Fairseq and return the result.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: Path to the data directory.\n",
    "    - ckpt: Checkpoint path for the Fairseq model.\n",
    "\n",
    "    Returns:\n",
    "    - content: Generated table content as a string.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONPATH\"] = \".\"\n",
    "\n",
    "    # fairseq-interactive command\n",
    "    cmd_fairseq = [\n",
    "        \"fairseq-interactive\", os.path.join(data_path, \"bins\"),\n",
    "        \"--path\", ckpt,\n",
    "        \"--beam\", str(DEFAULT_BEAM),\n",
    "        \"--remove-bpe\",\n",
    "        \"--buffer-size\", str(DEFAULT_BUFFER_SIZE),\n",
    "        \"--max-tokens\", str(DEFAULT_MAX_TOKENS),\n",
    "        \"--user-dir\", DEFAULT_USER_DIR,\n",
    "        \"--task\", DEFAULT_TASK,\n",
    "        \"--table-max-columns\", str(DEFAULT_TABLE_MAX_COLUMNS),\n",
    "        \"--unconstrained-decoding\"\n",
    "    ]\n",
    "    \n",
    "    # Run fairseq-interactive and redirect input/output\n",
    "    with open(os.path.join(data_path, \"test.bpe.text\"), \"r\") as input_file, \\\n",
    "         open(f\"{ckpt}.test_vanilla.out\", \"w\") as output_file:\n",
    "        try:\n",
    "            subprocess_result = subprocess.run(cmd_fairseq, stdin=input_file, stdout=output_file, check=True, text=True)\n",
    "            logging.info(subprocess_result.stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logging.error(f\"Error during Fairseq interactive: {e}\")\n",
    "            return str(e)\n",
    "\n",
    "    script_dir = os.getcwd()+\"/text_to_table/scripts/eval\"\n",
    "    # Convert fairseq output to text\n",
    "    convert_fairseq_output_to_text(script_dir, f\"{ckpt}.test_vanilla.out\")\n",
    "\n",
    "    # Read and return the content\n",
    "    result_file_path = os.path.join(os.getcwd()+\"/text_to_table/checkpoints\", \"checkpoint_average_best-3.pt.test_vanilla.out.text\")\n",
    "    try:\n",
    "        with open(result_file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Result file {result_file_path} not found!\")\n",
    "        return \"Result file not found!\"\n",
    "\n",
    "    return content\n",
    "\n",
    "def convert_fairseq_output_to_text(script_dir, filename):\n",
    "    \"\"\"\n",
    "    Convert Fairseq output to plain text.\n",
    "\n",
    "    Parameters:\n",
    "    - script_dir: Directory containing the necessary Python scripts.\n",
    "    - filename: File to be converted.\n",
    "    \"\"\"\n",
    "    # Run get_hypothesis.py\n",
    "    try:\n",
    "        subprocess_result = subprocess.run([\"python\", os.path.join(script_dir, \"get_hypothesis.py\"), filename, f\"{filename}.hyp\"], check=True, text=True)\n",
    "        logging.info(subprocess_result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Error during hypothesis extraction: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "    # Run gpt2_decode.py\n",
    "    try:\n",
    "        subprocess_result = subprocess.run([\"python\", os.path.join(script_dir, \"gpt2_decode.py\"), f\"{filename}.hyp\", f\"{filename}.text\"], check=True, text=True)\n",
    "        logging.info(subprocess_result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Error during GPT-2 decoding: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def convert_to_markdown(table_str):\n",
    "    \"\"\"Convert the given table string into markdown format.\"\"\"\n",
    "    rows = table_str.split(\"| <NEWLINE> |\")\n",
    "    \n",
    "    # Extract headers and values\n",
    "    headers = []\n",
    "    values = []\n",
    "    for row in rows:\n",
    "        parts = row.strip('|').split('|')\n",
    "        headers.append(parts[0].strip())\n",
    "        values.append(parts[1].strip())\n",
    "    \n",
    "    # Convert to markdown format\n",
    "    header_str = \"| \" + \" | \".join(headers) + \" |\"\n",
    "    separator_str = \"| \" + \" | \".join([\"-\"*len(header) for header in headers]) + \" |\"\n",
    "    value_str = \"| \" + \" | \".join(values) + \" |\"\n",
    "    \n",
    "    return \"\\n\".join([header_str, separator_str, value_str])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa798bd-ae67-41fe-b8e8-b658458b64e9",
   "metadata": {},
   "source": [
    "## Endpoint Function\n",
    "Finally, the generate_table_endpoint function combines all the steps to provide an endpoint for generating a table from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4513d60-2f46-481f-b895-f1aa68169604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_endpoint(text):\n",
    "    try:\n",
    "        # Preprocess the text\n",
    "        preprocess_path = preprocess_text(text, DATA_DIR)\n",
    "       \n",
    "        # Generate the table\n",
    "        table_content = table_generation(DATA_DIR, CKPT)\n",
    "\n",
    "        # Convert the table content to markdown\n",
    "        table_markdown = convert_to_markdown(table_content)\n",
    "\n",
    "        return table_markdown\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the entire process\n",
    "        logging.error(\"An error occurred during table generation:\", exc_info=True)\n",
    "        return str(e)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
